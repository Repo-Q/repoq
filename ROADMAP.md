# ROADMAP ‚Äî repoq 2.0 ‚Üí Production-Ready

**–¶–µ–ª—å**: –î–æ–≤–µ—Å—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–æ —É—Ä–æ–≤–Ω—è —Ñ–æ—Ä–º–∞–ª—å–Ω–æ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã —Å URPKS-–≥–∞—Ä–∞–Ω—Ç–∏—è–º–∏ (soundness, confluence, completeness, termination).

**–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è**: Œ£‚ÜíŒì‚Üíùí´‚ÜíŒõ‚ÜíR (—Å–∏–≥–Ω–∞—Ç—É—Ä–∞ ‚Üí –≥–µ–π—Ç—ã ‚Üí –æ–ø—Ü–∏–∏ ‚Üí –∞–≥—Ä–µ–≥–∞—Ü–∏—è ‚Üí —Ä–µ–∑—É–ª—å—Ç–∞—Ç)

---

## –§–∞–∑—ã —Ä–∞–∑–≤–∏—Ç–∏—è

```
Phase 0: CURRENT (v2.0)     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 70% –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
Phase 1: FOUNDATIONS        ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ (80% ‚Üí milestone M1)
Phase 2: FORMALIZATION      ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ (90% ‚Üí milestone M2)
Phase 3: VERIFICATION       ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ (95% ‚Üí milestone M3)
Phase 4: PRODUCTION         ‚îÅ‚îÅ‚îÅ (100% ‚Üí v3.0 GA)
```

---

## Phase 0: –¢–ï–ö–£–©–ï–ï –°–û–°–¢–û–Ø–ù–ò–ï (baseline)

### ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ
- [x] CLI —Å 3 —Ä–µ–∂–∏–º–∞–º–∏ (structure/history/full)
- [x] 6 –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ (Structure, History, Complexity, Weakness, Hotspots, CI/QM)
- [x] JSON-LD —ç–∫—Å–ø–æ—Ä—Ç —Å –æ–Ω—Ç–æ–ª–æ–≥–∏—è–º–∏ (PROV-O, OSLC, SPDX, FOAF, SDO)
- [x] TTL —ç–∫—Å–ø–æ—Ä—Ç (RDF/Turtle)
- [x] SHACL –≤–∞–ª–∏–¥–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- [x] Graphviz –≥—Ä–∞—Ñ—ã (dependencies, coupling)
- [x] Markdown –æ—Ç—á—ë—Ç—ã
- [x] –ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç

### ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–µ–ª—ã
- [ ] –§–æ—Ä–º–∞–ª—å–Ω–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–Ω—Ç–æ–ª–æ–≥–∏–∏ (OML/OWL2)
- [ ] –î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∑–≤—É–∫–æ–≤–æ—Å—Ç–∏ –º–∞–ø–ø–∏–Ω–≥–∞ (Python‚ÜíRDF)
- [ ] Self-hosting (–∞–Ω–∞–ª–∏–∑ —Å–∞–º–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞)
- [ ] Unit-—Ç–µ—Å—Ç—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ (coverage <10%)
- [ ] SPARQL-–ø—Ä–∏–º–µ—Ä—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
- [ ] Property-based —Ç–µ—Å—Ç—ã (–∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å, –¥–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º)
- [ ] –°—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏
- [ ] Deep merge –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (—Å–µ–π—á–∞—Å shallow)

---

## Phase 1: FOUNDATIONS (4 –Ω–µ–¥–µ–ª–∏) ‚Üí **Milestone M1**

**–¶–µ–ª—å**: –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

### Week 1-2: Testing Infrastructure
**–ì–µ–π—Ç**: `tests_min_cov: 0.8` ‚úÖ

#### Task 1.1: Unit-—Ç–µ—Å—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
```bash
tests/
  analyzers/
    test_structure.py         # StructureAnalyzer
    test_history.py           # HistoryAnalyzer
    test_complexity.py        # ComplexityAnalyzer
    test_weakness.py          # WeaknessAnalyzer
    test_hotspots.py          # HotspotsAnalyzer
    test_ci_qm.py             # CIQualityAnalyzer
  core/
    test_model.py             # dataclass invariants
    test_jsonld.py            # –º–∞–ø–ø–∏–Ω–≥ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏
    test_rdf_export.py        # TTL serialization
    test_repo_loader.py       # Git operations
  integration/
    test_pipelines.py         # end-to-end scenarios
    test_cli_modes.py         # CLI –∫–æ–º–∞–Ω–¥—ã
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] Coverage ‚â• 80% (pytest-cov)
- [ ] –í—Å–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã –∏–º–µ—é—Ç ‚â•5 unit-—Ç–µ—Å—Ç–æ–≤
- [ ] –¢–µ—Å—Ç—ã –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω—ã (—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–∫–∏ –¥–ª—è Git)

#### Task 1.2: Property-based —Ç–µ—Å—Ç—ã
```python
# tests/properties/test_analyzers_properties.py
from hypothesis import given, strategies as st

@given(st.text(), st.integers(min_value=0))
def test_structure_analyzer_idempotent(repo_path, seed):
    """–î–≤–æ–π–Ω–æ–π –∑–∞–ø—É—Å–∫ ‚Üí –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
    result1 = run_structure_twice(repo_path, seed)
    result2 = run_structure_twice(repo_path, seed)
    assert result1 == result2

@given(st.lists(st.text()))
def test_analyzers_commutative(analyzer_order):
    """–ü–æ—Ä—è–¥–æ–∫ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –≥—Ä–∞—Ñ (–∫–æ–º–º—É—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å)"""
    # ... –ø—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ coupling/dependencies –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] ‚â•10 property-—Ç–µ—Å—Ç–æ–≤ (Hypothesis)
- [ ] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
- [ ] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–º–º—É—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–≥–¥–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ)

### Week 3: Self-hosting
**–ì–µ–π—Ç**: `reflexive_completeness: true` ‚úÖ

#### Task 1.3: Bootstrap pipeline
```bash
# .github/workflows/self_analyze.yml
name: Self-Hosting Quality Check
on: [push, pull_request]
jobs:
  analyze-self:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: pip install -e ".[full]"
      - run: repoq full . -o artifacts/repoq_self.jsonld --ttl artifacts/repoq_self.ttl --validate-shapes
      - run: python scripts/assert_quality_gates.py artifacts/repoq_self.jsonld
      - uses: actions/upload-artifact@v3
        with:
          name: self-analysis
          path: artifacts/
```

```python
# scripts/assert_quality_gates.py
import json, sys

data = json.load(open("artifacts/repoq_self.jsonld"))
issues_high = [i for i in data.get("issues", []) if i.get("severity", {}).get("@id", "").endswith("Critical")]

if len(issues_high) > 0:
    print(f"FAIL: {len(issues_high)} high-severity issues in repoq itself!")
    sys.exit(1)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
assert data.get("modules"), "No modules found"
assert len(data.get("contributors", [])) > 0, "No contributors"
print("‚úÖ Self-analysis passed")
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] CI –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç `repoq full .`
- [ ] –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è (repoq_self.jsonld/ttl)
- [ ] Fail –ø—Ä–∏ high-severity issues
- [ ] Badge –≤ README —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

#### Task 1.4: Deep merge –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
```python
# repoq/core/jsonld.py
def _deep_merge_contexts(base: dict, override: dict, path: str = "") -> dict:
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π merge —Å conflict detection"""
    conflicts = []
    result = base.copy()
    
    for key, value in override.items():
        if key in result:
            if isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = _deep_merge_contexts(result[key], value, f"{path}.{key}")
            elif result[key] != value:
                conflicts.append(f"{path}.{key}: {result[key]} vs {value}")
                result[key] = value  # override wins
        else:
            result[key] = value
    
    if conflicts:
        logger.warning(f"Context merge conflicts: {conflicts}")
    
    return result
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω `_deep_merge_contexts`
- [ ] –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
- [ ] –¢–µ—Å—Ç—ã –¥–ª—è corner cases (—Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ —Å—Å—ã–ª–∫–∏)

### Week 4: Documentation
**–ì–µ–π—Ç**: `quality: linters + docs` ‚úÖ

#### Task 1.5: SPARQL –ø—Ä–∏–º–µ—Ä—ã
```bash
examples/
  queries/
    01_project_overview.rq
    02_top_hotspots.rq
    03_authors_by_commits.rq
    04_coupling_matrix.rq
    05_issues_by_severity.rq
    06_test_coverage.rq
    07_dependencies_graph.rq
    08_prov_lineage.rq
    09_oslc_compliance.rq
    10_spdx_checksums.rq
  notebooks/
    analyze_with_sparql.ipynb
  data/
    sample_repoq_output.ttl
```

**–ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞**:
```sparql
# examples/queries/02_top_hotspots.rq
PREFIX repo: <http://example.org/repo#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?file ?hotness ?complexity ?churn
WHERE {
  ?file a repo:File ;
        repo:hotness ?hotness ;
        repo:complexity ?complexity ;
        repo:codeChurn ?churn .
}
ORDER BY DESC(?hotness)
LIMIT 10
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] ‚â•10 SPARQL –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
- [ ] Jupyter notebook —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
- [ ] README —Å–µ–∫—Ü–∏—è "Querying RDF Graph"

#### Task 1.6: –õ–∏–Ω—Ç–µ—Ä—ã –∏ CI checks
```toml
# pyproject.toml
[tool.ruff]
line-length = 120
target-version = "py39"
select = ["E", "F", "I", "N", "UP", "YTT", "S", "BLE", "FBT", "B", "A", "COM", "C4", "DTZ", "T10", "DJ", "EM", "EXE", "ISC", "ICN", "G", "INP", "PIE", "T20", "PYI", "PT", "Q", "RSE", "RET", "SLF", "SIM", "TID", "TCH", "INT", "ARG", "PTH", "ERA", "PD", "PGH", "PL", "TRY", "NPY", "RUF"]

[tool.mypy]
strict = true
warn_unused_ignores = true
disallow_untyped_defs = true
```

```yaml
# .github/workflows/ci.yml
name: CI
on: [push, pull_request]
jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: {python-version: "3.11"}
      - run: pip install -e ".[dev]"
      - run: ruff check .
      - run: black --check .
      - run: mypy repoq/
  
  test:
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12"]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: {python-version: "${{ matrix.python-version }}"}
      - run: pip install -e ".[full,dev]"
      - run: pytest --cov=repoq --cov-report=xml
      - uses: codecov/codecov-action@v3
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] CI –ø—Ä–æ—Ö–æ–¥–∏—Ç (lint + test + coverage)
- [ ] Codecov badge ‚â•80%
- [ ] mypy strict mode –±–µ–∑ –æ—à–∏–±–æ–∫

---

## Phase 2: FORMALIZATION (6 –Ω–µ–¥–µ–ª—å) ‚Üí **Milestone M2**

**–¶–µ–ª—å**: –§–æ—Ä–º–∞–ª—å–Ω–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–Ω—Ç–æ–ª–æ–≥–∏–∏ –∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∑–≤—É–∫–æ–≤–æ—Å—Ç–∏

### Week 5-6: OML/OWL2 Ontology
**–ì–µ–π—Ç**: `soundness: formal_spec` ‚úÖ

#### Task 2.1: OML —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è
```bash
ontologies/
  repoq_core.oml            # –ë–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã (Project, Module, File, Person)
  repoq_prov.oml            # PROV-O extension
  repoq_oslc.oml            # OSLC CM/QM/Config alignment
  repoq_spdx.oml            # SPDX File/Checksum
  repoq_metrics.oml         # –ú–µ—Ç—Ä–∏–∫–∏ (complexity, hotness, coupling)
  mappings/
    python_to_rdf.oml       # –ú–∞–ø–ø–∏–Ω–≥ Python dataclasses ‚Üí RDF
```

**–ü—Ä–∏–º–µ—Ä OML**:
```oml
@dc:title "repoq Core Ontology"
vocabulary <http://example.org/repoq/core#> as repoq {
  
  extends <http://www.w3.org/ns/prov#> as prov
  extends <http://spdx.org/rdf/terms#> as spdx
  
  concept Project :> prov:Entity [
    restricts all relation containsModule to Module
    restricts all relation containsFile to File
  ]
  
  concept File :> spdx:File, prov:Entity [
    restricts some scalar property linesOfCode to xsd:nonNegativeInteger
    restricts some scalar property complexity to xsd:decimal
    restricts all relation contributedBy to Person
  ]
  
  concept Person :> prov:Agent
  
  scalar property hotness :> [
    domain File
    range xsd:decimal
    functional
  ]
  
  // –ê–∫—Å–∏–æ–º—ã
  rule HotnessNonNegative [
    hotness(?f, ?h) -> greaterThanOrEqual(?h, 0.0)
  ]
}
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] ‚â•5 OML –º–æ–¥—É–ª–µ–π
- [ ] –ê–∫—Å–∏–æ–º—ã –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫
- [ ] –í–∞–ª–∏–¥–∞—Ü–∏—è OML —á–µ—Ä–µ–∑ Rosetta CLI

#### Task 2.2: SHACL by default + —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ shapes
```turtle
# shapes/repoq_advanced.ttl
@prefix sh: <http://www.w3.org/ns/shacl#> .
@prefix repo: <http://example.org/repo#> .

repo:FileShape a sh:NodeShape ;
  sh:targetClass repo:File ;
  sh:property [
    sh:path repo:linesOfCode ;
    sh:minInclusive 0 ;
    sh:datatype xsd:nonNegativeInteger ;
  ] ;
  sh:property [
    sh:path repo:hotness ;
    sh:minInclusive 0.0 ;
    sh:maxInclusive 1.0 ;
  ] ;
  sh:property [
    sh:path repo:complexity ;
    sh:minInclusive 0.0 ;
  ] .

repo:PersonShape a sh:NodeShape ;
  sh:targetClass foaf:Person ;
  sh:property [
    sh:path foaf:name ;
    sh:minCount 1 ;
    sh:datatype xsd:string ;
  ] ;
  sh:property [
    sh:path repo:commits ;
    sh:minInclusive 0 ;
  ] .
```

```python
# repoq/config.py
@dataclass
class AnalyzeConfig:
    validate_shapes: bool = True  # ‚Üê –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–∫–ª—é—á–µ–Ω–æ
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] SHACL –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞ by default
- [ ] ‚â•20 shapes (–ø–æ–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤)
- [ ] CI fail –ø—Ä–∏ SHACL violations

### Week 7-8: Lean4 Proofs
**–ì–µ–π—Ç**: `soundness: verified_mapping` ‚úÖ

#### Task 2.3: Lean4 —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –º–∞–ø–ø–∏–Ω–≥–∞
```bash
proofs/
  RepoqCore.lean            # –ë–∞–∑–æ–≤—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
  Mapping.lean              # –î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ Python‚ÜíRDF
  Soundness.lean            # Soundness —Ç–µ–æ—Ä–µ–º–∞
  Completeness.lean         # Completeness (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è)
  Tests.lean                # QuickCheck-like property tests
```

**–ü—Ä–∏–º–µ—Ä Lean4 –∫–æ–¥–∞**:
```lean
-- proofs/RepoqCore.lean
import Mathlib.Data.Finset.Basic
import Mathlib.Data.Real.Basic

structure File where
  path : String
  loc : Nat
  complexity : Option Real
  hotness : Option Real
  deriving Repr

-- –ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç: hotness –≤ [0, 1]
def File.valid (f : File) : Prop :=
  match f.hotness with
  | none => True
  | some h => 0 ‚â§ h ‚àß h ‚â§ 1

-- proofs/Mapping.lean
def pythonFileToRDF (f : File) : RDFGraph :=
  { triples := [
      (f.path, "rdf:type", "repo:File"),
      (f.path, "repo:linesOfCode", toString f.loc),
      -- ...
    ]
  }

-- Soundness: –≤–∞–ª–∏–¥–Ω—ã–π Python File ‚Üí –≤–∞–ª–∏–¥–Ω—ã–π RDF –≥—Ä–∞—Ñ
theorem mapping_preserves_validity (f : File) (h : f.valid) :
  (pythonFileToRDF f).satisfies repo_shapes := by
  unfold pythonFileToRDF
  unfold File.valid at h
  cases f.hotness with
  | none => simp; sorry  -- trivial case
  | some hval =>
    cases h with
    | intro h1 h2 =>
      -- –¥–æ–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ RDF triple (f.path, "repo:hotness", hval) —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç SHACL
      sorry
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] –§–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ dataclasses (Project, File, Module, Person)
- [ ] –î–æ–∫–∞–∑–∞–Ω–∞ —Ç–µ–æ—Ä–µ–º–∞ `mapping_preserves_validity`
- [ ] CI –ø—Ä–æ–≤–µ—Ä—è–µ—Ç Lean4 –ø—Ä–æ–µ–∫—Ç—ã (`lake build`)

### Week 9: –°—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏
**–ì–µ–π—Ç**: `reflexive_completeness: stratified_reflection` ‚úÖ

#### Task 2.4: –£—Ä–æ–≤–Ω–∏ –≤—Å–µ–ª–µ–Ω–Ω—ã—Ö –¥–ª—è –º–µ—Ç–∞–∞–Ω–∞–ª–∏–∑–∞
```python
# repoq/meta/levels.py
from enum import IntEnum

class AnalysisLevel(IntEnum):
    """–°—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π –∞–Ω–∞–ª–∏–∑–∞ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–¥–æ–∫—Å—ã self-reference)"""
    L0_CODE = 0          # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π –∫–æ–¥ (user projects)
    L1_ANALYZER = 1      # –ö–æ–¥ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ (repoq itself)
    L2_META_ANALYZER = 2 # –ê–Ω–∞–ª–∏–∑ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ (–±—É–¥—É—â–µ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ)

@dataclass
class Project:
    # ...
    analysis_level: AnalysisLevel = AnalysisLevel.L0_CODE
    
    def analyze_self(self) -> "Project":
        """Self-hosting: –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–∞–º –ø—Ä–æ–µ–∫—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ L1"""
        if self.analysis_level >= AnalysisLevel.L1_ANALYZER:
            raise ValueError("Cannot self-analyze beyond L1 (prevents infinite regress)")
        
        # ... –∑–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —Å level=L1
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] –í–≤–µ–¥–µ–Ω—ã —É—Ä–æ–≤–Ω–∏ 0/1/2
- [ ] –ó–∞–ø—Ä–µ—Ç –Ω–∞ –∞–Ω–∞–ª–∏–∑ —É—Ä–æ–≤–Ω—è ‚â•L2 (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø–∞—Ä–∞–¥–æ–∫—Å–æ–≤)
- [ ] –¢–µ—Å—Ç—ã –¥–ª—è cross-level analysis

### Week 10: Confluence check
**–ì–µ–π—Ç**: `confluence: orthogonality` ‚úÖ

#### Task 2.5: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
```python
# tests/confluence/test_analyzer_independence.py
import pytest
from itertools import permutations

def test_analyzers_orthogonal():
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—Ç (–ª–æ–∫–∞–ª—å–Ω–∞—è confluence)"""
    repo = create_test_repo()
    project = Project(...)
    
    # –í—Å–µ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–æ—Ä—è–¥–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
    analyzers = [StructureAnalyzer(), HistoryAnalyzer(), ComplexityAnalyzer()]
    
    results = []
    for order in permutations(analyzers):
        p = deepcopy(project)
        for analyzer in order:
            analyzer.run(p, repo, cfg)
        results.append(p.to_dict())
    
    # –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã (–º–æ–¥—É–ª–æ ordering)
    canonical = normalize(results[0])
    for r in results[1:]:
        assert normalize(r) == canonical, "Analyzers not orthogonal!"

def normalize(data: dict) -> dict:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (sorting —Å–ø–∏—Å–∫–æ–≤ –∏ —Ç.–ø.)"""
    # ...
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] –¢–µ—Å—Ç –Ω–∞ –≤—Å–µ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
- [ ] –î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (–Ω–µ—Ç –≤–∑–∞–∏–º–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
- [ ] –ï—Å–ª–∏ –Ω–µ–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã ‚Üí —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞

---

## Phase 3: VERIFICATION (4 –Ω–µ–¥–µ–ª–∏) ‚Üí **Milestone M3**

**–¶–µ–ª—å**: –ü–æ–ª–Ω–∞—è —Ñ–æ—Ä–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å

### Week 11-12: Formal Verification Suite
**–ì–µ–π—Ç**: `all_gates: green` ‚úÖ

#### Task 3.1: TLA+ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
```tla
---- MODULE RepoqPipeline ----
EXTENDS Naturals, Sequences, FiniteSets

CONSTANTS Analyzers, MaxFiles
VARIABLES project, currentAnalyzer, filesProcessed

Init ==
  /\ project = [files |-> {}, modules |-> {}, issues |-> {}]
  /\ currentAnalyzer = 1
  /\ filesProcessed = 0

RunAnalyzer(a) ==
  /\ currentAnalyzer = a
  /\ filesProcessed < MaxFiles
  /\ project' = [project EXCEPT 
       !.files = @ \union AnalyzeFiles(a),
       !.issues = @ \union DetectIssues(a)]
  /\ filesProcessed' = filesProcessed + 1
  /\ currentAnalyzer' = IF a < Len(Analyzers) THEN a + 1 ELSE a

Termination == filesProcessed >= MaxFiles \/ currentAnalyzer > Len(Analyzers)

Invariant ==
  /\ filesProcessed <= MaxFiles  \* –≤—Å–µ–≥–¥–∞ —Ç–µ—Ä–º–∏–Ω–∏—Ä—É–µ–º
  /\ Cardinality(project.issues) < 10000  \* –Ω–µ –≤–∑—Ä—ã–≤ –ø–∞–º—è—Ç–∏

Spec == Init /\ [][RunAnalyzer(currentAnalyzer)]_<<project, currentAnalyzer, filesProcessed>> /\ WF_<<...>>(Termination)
====
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] TLA+ spec –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞
- [ ] Model checking —á–µ—Ä–µ–∑ TLC (proof of termination)
- [ ] Invariants –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã (no deadlocks, bounded memory)

#### Task 3.2: Coq/Isabelle —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (optional, high-value)
```coq
(* proofs/repoq_soundness.v *)
Require Import Coq.Strings.String.
Require Import Coq.Lists.List.
Import ListNotations.

Inductive RDFTriple : Type :=
  | triple : string -> string -> string -> RDFTriple.

Definition RDFGraph := list RDFTriple.

Inductive File : Type :=
  | mkFile : string -> nat -> option Q -> File.  (* path, LOC, complexity *)

Definition fileToRDF (f : File) : RDFGraph :=
  match f with
  | mkFile p loc cplx =>
      [ triple p "rdf:type" "repo:File" ;
        triple p "repo:linesOfCode" (natToString loc) ]
  end.

(* Soundness: –≤—Å–µ–≥–¥–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–æ—Ç—è –±—ã rdf:type *)
Theorem fileToRDF_has_type : forall f,
  exists g, In (triple (filePath f) "rdf:type" "repo:File") (fileToRDF f).
Proof.
  intros. destruct f as [p loc cplx]. simpl.
  exists (triple p "rdf:type" "repo:File"). left. reflexivity.
Qed.
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] –ë–∞–∑–æ–≤–∞—è Coq —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–º–∞–ø–ø–∏–Ω–≥ + soundness)
- [ ] Proof-carrying code (—ç–∫—Å–ø–æ—Ä—Ç —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤)

### Week 13: Performance & Scalability
**–ì–µ–π—Ç**: `performance: production_ready` ‚úÖ

#### Task 3.3: Benchmarks
```python
# benchmarks/bench_analyzers.py
import pytest
from repoq.pipeline import run_pipeline

@pytest.mark.benchmark(group="analyzers")
def test_structure_10k_files(benchmark, large_repo):
    """–°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ 10K —Ñ–∞–π–ª–æ–≤ < 60 —Å–µ–∫"""
    result = benchmark(run_pipeline, mode="structure", repo=large_repo, max_files=10000)
    assert result.duration < 60.0

@pytest.mark.benchmark(group="history")
def test_history_100k_commits(benchmark, huge_repo):
    """–ò—Å—Ç–æ—Ä–∏—è 100K –∫–æ–º–º–∏—Ç–æ–≤ < 300 —Å–µ–∫"""
    result = benchmark(run_pipeline, mode="history", repo=huge_repo)
    assert result.duration < 300.0
```

```bash
# CI benchmark regression tracking
pytest benchmarks/ --benchmark-autosave --benchmark-compare=last
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] Benchmarks –¥–ª—è –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
- [ ] Regression tracking –≤ CI
- [ ] –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —É–∑–∫–∏—Ö –º–µ—Å—Ç (profiling ‚Üí pypy/cython –¥–ª—è hotspots)

#### Task 3.4: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (parallel execution)
```python
# repoq/pipeline.py
from concurrent.futures import ProcessPoolExecutor

def run_pipeline_parallel(project: Project, repo_dir: str, cfg: AnalyzeConfig) -> None:
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"""
    
    # Group 1: –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ (–º–æ–∂–Ω–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
    independent = [StructureAnalyzer(), ComplexityAnalyzer(), WeaknessAnalyzer()]
    
    with ProcessPoolExecutor(max_workers=cfg.max_workers) as executor:
        futures = [executor.submit(a.run, project, repo_dir, cfg) for a in independent]
        for f in futures:
            f.result()  # –∂–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    
    # Group 2: –∑–∞–≤–∏—Å–∏–º—ã–µ –æ—Ç Group 1 (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)
    HistoryAnalyzer().run(project, repo_dir, cfg)
    HotspotsAnalyzer().run(project, repo_dir, cfg)
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º (--parallel —Ñ–ª–∞–≥)
- [ ] Speedup ‚â•2x –Ω–∞ 4-core –º–∞—à–∏–Ω–µ
- [ ] Thread-safety –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤

### Week 14: Security & Hardening
**–ì–µ–π—Ç**: `security: production_hardened` ‚úÖ

#### Task 3.5: Security audit
```bash
# CI security checks
pip install bandit safety semgrep
bandit -r repoq/ -f json -o security_report.json
safety check --json
semgrep --config=auto repoq/
```

**Checklist**:
- [ ] –ù–µ—Ç eval/exec/pickle –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
- [ ] –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≤—Ö–æ–¥–æ–≤ (paths, URLs, globs)
- [ ] Rate limiting –¥–ª—è Git clone (–∑–∞—â–∏—Ç–∞ –æ—Ç DoS)
- [ ] Sandboxing –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ–¥–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤

#### Task 3.6: Error handling & resilience
```python
# repoq/core/resilience.py
from tenacity import retry, stop_after_attempt, wait_exponential

class AnalyzerError(Exception):
    """–ë–∞–∑–æ–≤–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    pass

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def safe_git_clone(url: str, dest: str) -> str:
    """Retry-–ª–æ–≥–∏–∫–∞ –¥–ª—è Git –æ–ø–µ—Ä–∞—Ü–∏–π"""
    try:
        return git.Repo.clone_from(url, dest)
    except git.GitCommandError as e:
        logger.warning(f"Git clone failed: {e}, retrying...")
        raise

def run_analyzer_safe(analyzer: Analyzer, project: Project, repo_dir: str, cfg: AnalyzeConfig) -> None:
    """–û–±—ë—Ä—Ç–∫–∞ —Å graceful degradation"""
    try:
        analyzer.run(project, repo_dir, cfg)
    except Exception as e:
        logger.error(f"{analyzer.__class__.__name__} failed: {e}")
        project.issues[f"analyzer_failure_{analyzer.__class__.__name__}"] = Issue(
            id=f"error:{analyzer.__class__.__name__}",
            type="oslc_cm:Defect",
            description=f"Analyzer crashed: {e}",
            severity="high"
        )
        if cfg.fail_fast:
            raise
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] Retry –¥–ª—è –≤—Å–µ—Ö —Å–µ—Ç–µ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- [ ] Graceful degradation (–ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ 1 –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞)
- [ ] Structured logging (JSON logs –¥–ª—è production)

---

## Phase 4: PRODUCTION (2 –Ω–µ–¥–µ–ª–∏) ‚Üí **v3.0 GA**

**–¶–µ–ª—å**: –†–µ–ª–∏–∑ production-ready –≤–µ—Ä—Å–∏–∏ —Å –ø–æ–ª–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π

### Week 15: Production Deployment
**–ì–µ–π—Ç**: `deployment: automated` ‚úÖ

#### Task 4.1: Docker + Helm
```dockerfile
# Dockerfile
FROM python:3.11-slim
WORKDIR /app
RUN apt-get update && apt-get install -y git graphviz && rm -rf /var/lib/apt/lists/*
COPY . /app
RUN pip install --no-cache-dir -e ".[full]"
ENTRYPOINT ["repoq"]
```

```yaml
# helm/repoq/values.yaml
image:
  repository: ghcr.io/yourorg/repoq
  tag: "3.0.0"
  pullPolicy: IfNotPresent

resources:
  limits:
    cpu: 4
    memory: 8Gi
  requests:
    cpu: 2
    memory: 4Gi

persistence:
  enabled: true
  size: 50Gi
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] Docker image < 500MB
- [ ] Helm chart –¥–ª—è Kubernetes
- [ ] CI –ø—É–±–ª–∏–∫—É–µ—Ç –æ–±—Ä–∞–∑—ã –≤ GHCR

#### Task 4.2: SaaS API (optional)
```python
# repoq/api/server.py
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel

app = FastAPI(title="repoq API", version="3.0.0")

class AnalysisRequest(BaseModel):
    repository_url: str
    mode: str = "full"
    options: dict = {}

@app.post("/analyze")
async def analyze(req: AnalysisRequest, background_tasks: BackgroundTasks):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è"""
    task_id = uuid4()
    background_tasks.add_task(run_analysis_task, task_id, req)
    return {"task_id": task_id, "status": "queued"}

@app.get("/results/{task_id}")
async def get_results(task_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"""
    # ... retrieve from storage
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] REST API (FastAPI)
- [ ] Async queue (Celery/RQ)
- [ ] S3 storage –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### Week 16: Documentation & Release
**–ì–µ–π—Ç**: `documentation: complete` ‚úÖ

#### Task 4.3: –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
```bash
docs/
  index.md                  # –ì–ª–∞–≤–Ω–∞—è
  getting-started.md        # Quick start
  architecture/
    overview.md
    analyzers.md
    ontology.md
    pipeline.md
  api/
    cli-reference.md
    python-api.md
    rest-api.md
  guides/
    ci-integration.md
    sparql-queries.md
    custom-analyzers.md
    performance-tuning.md
  verification/
    formal-proofs.md        # Lean4/Coq –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
    shacl-shapes.md
    testing-strategy.md
  deployment/
    docker.md
    kubernetes.md
    production-checklist.md
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] MkDocs site (readthedocs theme)
- [ ] API reference –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è (sphinx-apidoc)
- [ ] Video tutorial (5 –º–∏–Ω)
- [ ] Case studies (‚â•3 –ø—Ä–∏–º–µ—Ä–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö)

#### Task 4.4: Release –ø—Ä–æ—Ü–µ—Å—Å
```yaml
# .github/workflows/release.yml
name: Release
on:
  push:
    tags: ['v*']
jobs:
  release:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: python -m build
      - run: twine upload dist/*
        env:
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ secrets.PYPI_TOKEN }}
      - uses: docker/build-push-action@v5
        with:
          push: true
          tags: ghcr.io/yourorg/repoq:${{ github.ref_name }}
      - uses: ncipollo/release-action@v1
        with:
          artifacts: "dist/*"
          generateReleaseNotes: true
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏**:
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è –≤ PyPI
- [ ] GitHub Release —Å changelog
- [ ] Docker image tagged
- [ ] Announcement (blog post, Twitter, HN)

---

## –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞ (Gate Criteria)

| –ì–µ–π—Ç | Phase 0 | M1 | M2 | M3 | v3.0 |
|------|---------|----|----|----|----|
| **Soundness** | ‚ö†Ô∏è 40% | 60% | ‚úÖ 90% | ‚úÖ 95% | ‚úÖ 100% |
| **Completeness** | ‚ö†Ô∏è 50% | 70% | ‚úÖ 85% | ‚úÖ 90% | ‚úÖ 95% |
| **Confluence** | ‚ö†Ô∏è 60% | ‚úÖ 80% | ‚úÖ 90% | ‚úÖ 95% | ‚úÖ 100% |
| **Termination** | ‚úÖ 80% | ‚úÖ 90% | ‚úÖ 95% | ‚úÖ 100% | ‚úÖ 100% |
| **Test Coverage** | ‚ùå 5% | ‚úÖ 80% | ‚úÖ 85% | ‚úÖ 90% | ‚úÖ 95% |
| **Documentation** | ‚ö†Ô∏è 30% | 50% | 70% | ‚úÖ 90% | ‚úÖ 100% |
| **Performance** | ‚ö†Ô∏è 60% | 70% | ‚úÖ 80% | ‚úÖ 95% | ‚úÖ 100% |
| **Security** | ‚ö†Ô∏è 50% | 60% | 70% | ‚úÖ 95% | ‚úÖ 100% |

---

## –†–µ—Å—É—Ä—Å—ã –∏ –∫–æ–º–∞–Ω–¥–∞

### –¢—Ä–µ–±—É–µ–º—ã–µ —Ä–æ–ª–∏
- **Lead Engineer** (1 FTE) ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∫–æ–¥-—Ä–µ–≤—å—é
- **Formal Methods Expert** (0.5 FTE) ‚Äî Lean4/Coq/TLA+
- **DevOps Engineer** (0.3 FTE) ‚Äî CI/CD, Kubernetes
- **Technical Writer** (0.2 FTE) ‚Äî –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### –ë—é–¥–∂–µ—Ç –≤—Ä–µ–º–µ–Ω–∏
- Phase 1: 4 –Ω–µ–¥–µ–ª–∏ √ó 1.5 FTE = 6 person-weeks
- Phase 2: 6 –Ω–µ–¥–µ–ª—å √ó 2 FTE = 12 person-weeks
- Phase 3: 4 –Ω–µ–¥–µ–ª–∏ √ó 2 FTE = 8 person-weeks
- Phase 4: 2 –Ω–µ–¥–µ–ª–∏ √ó 1.5 FTE = 3 person-weeks
- **–ò–¢–û–ì–û**: ~29 person-weeks ‚âà 7 calendar months

---

## –†–∏—Å–∫–∏ –∏ –º–∏—Ç–∏–≥–∞—Ü–∏–∏

| –†–∏—Å–∫ | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å | –í–ª–∏—è–Ω–∏–µ | –ú–∏—Ç–∏–≥–∞—Ü–∏—è |
|------|-------------|---------|-----------|
| Lean4 –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω—ã | HIGH | MEDIUM | –£–ø—Ä–æ—Å—Ç–∏—Ç—å –¥–æ Coq/QuickCheck; —Ñ–æ–∫—É—Å –Ω–∞ key properties |
| OML —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç edge cases | MEDIUM | HIGH | –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ + —Ç–µ—Å—Ç—ã –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö |
| Performance –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö —Ä–µ–ø–æ | MEDIUM | MEDIUM | –†–∞–Ω–Ω–µ–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ + incremental analysis |
| SHACL shapes –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—Ç —Å legacy –¥–∞–Ω–Ω—ã–º–∏ | LOW | HIGH | Versioning –æ–Ω—Ç–æ–ª–æ–≥–∏–∏ (v2 vs v3 contexts) |
| –ö–æ–º–∞–Ω–¥–∞ –Ω–µ–¥–æ–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–±—ä—ë–º —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ | HIGH | HIGH | Weekly checkpoints, MVP-–ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º |

---

## –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è (Definition of Done)

‚úÖ **v3.0 GA —Å—á–∏—Ç–∞–µ—Ç—Å—è –≥–æ—Ç–æ–≤–æ–π, –µ—Å–ª–∏**:

1. **–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è**:
   - [ ] –í—Å–µ –≥–µ–π—Ç—ã –∑–µ–ª—ë–Ω—ã–µ (soundness/confluence/completeness/termination)
   - [ ] Lean4 —Ç–µ–æ—Ä–µ–º–∞ `mapping_preserves_validity` –¥–æ–∫–∞–∑–∞–Ω–∞
   - [ ] TLA+ model checking –ø—Ä–æ—Ö–æ–¥–∏—Ç –±–µ–∑ violations
   - [ ] Coverage ‚â•90%

2. **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**:
   - [ ] 10K —Ñ–∞–π–ª–æ–≤ < 60 —Å–µ–∫
   - [ ] 100K –∫–æ–º–º–∏—Ç–æ–≤ < 5 –º–∏–Ω
   - [ ] Memory < 8GB –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤

3. **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**:
   - [ ] –ü–æ–ª–Ω–∞—è docs site (MkDocs)
   - [ ] ‚â•10 SPARQL –ø—Ä–∏–º–µ—Ä–æ–≤
   - [ ] Video tutorial
   - [ ] ‚â•3 case studies

4. **Deployment**:
   - [ ] Docker image –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω
   - [ ] PyPI package released
   - [ ] Helm chart –≥–æ—Ç–æ–≤
   - [ ] Production checklist –ø—Ä–æ–π–¥–µ–Ω

5. **Community**:
   - [ ] ‚â•100 GitHub stars
   - [ ] ‚â•10 contributors
   - [ ] ‚â•5 issues closed
   - [ ] Mention –≤ ‚â•2 —Å—Ç–∞—Ç—å—è—Ö/–±–ª–æ–≥–∞—Ö

---

## Next Steps (–Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è)

```bash
# Week 1 Sprint Plan
git checkout -b phase1/testing-infra

# Day 1-2: Setup pytest infrastructure
mkdir -p tests/{analyzers,core,integration,properties,confluence}
touch tests/analyzers/test_{structure,history,complexity,weakness,hotspots,ci_qm}.py

# Day 3-4: Implement first 20 unit tests
# ... (focus on StructureAnalyzer + ComplexityAnalyzer)

# Day 5: Setup CI
cp examples/ci.yml.template .github/workflows/ci.yml
# Add codecov, pytest, ruff, mypy

# End of Week 1: PR with ‚â•50% coverage increase
```

**–ó–∞–ø—É—Å—Ç–∏—Ç—å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ**:
```bash
pip install -e ".[dev]"
pytest --cov=repoq --cov-report=term-missing  # baseline coverage
ruff check . --fix
mypy repoq/ --install-types
```

---

**–°—Ç–∞—Ç—É—Å**: ROADMAP —É—Ç–≤–µ—Ä–∂–¥—ë–Ω. –ù–∞—á–∞–ª–æ Phase 1 ‚Äî –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ.  
**Owner**: Lead Engineer  
**Review cadence**: –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–µ sync-up –ø–æ –ø—è—Ç–Ω–∏—Ü–∞–º  
**Success metric**: v3.0 GA –≤ production –∫ Q2 2026
