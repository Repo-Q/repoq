# Plan —É–ª—É—á—à–µ–Ω–∏–π RepoQ: –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π roadmap

**–î–∞—Ç–∞**: 2025-10-22  
**–ö–æ–Ω—Ç–µ–∫—Å—Ç**: Post Option C implementation (per-function metrics)  
**–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è**: Œ£‚ÜíŒì‚Üíùí´‚ÜíŒõ‚ÜíR –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è

---

## [Œ£] Signature: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã

### ‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ (v0.4.0)

**Core Features**:

- ‚úÖ Structure analysis (files, modules, dependencies)
- ‚úÖ Complexity metrics (cyclomatic complexity, MI)
- ‚úÖ Git history (authorship, churn, temporal coupling)
- ‚úÖ Hotspots detection (high-churn + high-complexity)
- ‚úÖ Semantic export (JSON-LD, RDF/Turtle, W3C ontologies)
- ‚úÖ Refactoring plan generation (PCE algorithm)
- ‚úÖ Quality gates (CI/CD integration)
- ‚úÖ **Per-function metrics** (NEW! Functions with CCN, LOC, line ranges)

**Testing**:

- ‚úÖ 295 tests collected
- ‚úÖ Coverage: 63%
- ‚úÖ CI: GitHub Actions

**Documentation**:

- ‚úÖ README with quick start
- ‚úÖ Dogfooding meta-level report
- ‚úÖ Task #2 failure analysis (Œ£‚ÜíŒì‚Üíùí´‚ÜíŒõ‚ÜíR)

### üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã

**Problem 1: High complexity –≤ core –º–æ–¥—É–ª—è—Ö**

```
refactoring-plan-v2.md (Top-5 CRITICAL):
1. repoq/analyzers/history.py:     CCN=35 (_run_pydriller), CCN=30 (_run_git)
2. repoq/core/jsonld.py:           CCN=33 (to_jsonld)
3. repoq/cli.py:                   CCN=26 (_run_command), CCN=16 (_run_trs_verification)
4. repoq/gate.py:                  CCN=23 (format_gate_report)
5. repoq/refactoring.py:           CCN=21 (generate_plan)

Total Expected ŒîQ: +589.0 points
```

**Problem 2: tmp/ pollution risk**

- ‚ùå `--exclude` –Ω–µ –≤—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
- ‚ùå –ù–µ—Ç –∞–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∏ tmp/ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
- ‚ùå –ù–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ stale data

**Problem 3: –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ per-function ŒîQ estimation**

- ‚ùå Plan –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ñ–∞–π–ª–æ–≤—ã–π ŒîQ, –Ω–µ function-level
- ‚ùå –ù–µ—Ç –æ—Ü–µ–Ω–∫–∏ "—Å–∫–æ–ª—å–∫–æ –¥–∞—Å—Ç —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ —Ñ—É–Ω–∫—Ü–∏–∏ X"
- ‚ùå –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Ñ–∞–π–ª–∞—Ö, –Ω–µ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è—Ö

**Problem 4: Limited IDE integration**

- ‚ùå –ù–µ—Ç quick-jump links –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π
- ‚ùå –ù–µ—Ç inline recommendations –≤ IDE
- ‚ùå –ù–µ—Ç auto-fix suggestions

**Problem 5: Single-language support**

- ‚úÖ Python (lizard)
- ‚ùå JavaScript/TypeScript (–Ω—É–∂–µ–Ω ESLint/TSLint)
- ‚ùå Java (–Ω—É–∂–µ–Ω PMD/Checkstyle)
- ‚ùå Go, Rust, C++, etc.

---

## [Œì] Gates: –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —É–ª—É—á—à–µ–Ω–∏–π

–ö–∞–∂–¥–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–æ–π—Ç–∏ –≥–µ–π—Ç—ã:

### Gate 1: Soundness (–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å)

- ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–µ –∫–æ–¥–∞
- ‚úÖ –ù–µ—Ç false positives/negatives
- ‚úÖ –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### Gate 2: Performance (–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å)

- ‚úÖ Analysis time: O(N) –≥–¥–µ N = LOC
- ‚úÖ Memory usage: <512MB –¥–ª—è –ø—Ä–æ–µ–∫—Ç–æ–≤ –¥–æ 100K LOC
- ‚úÖ No blocking operations –±–µ–∑ timeout

### Gate 3: Usability (—É–¥–æ–±—Å—Ç–≤–æ)

- ‚úÖ Actionable recommendations (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è)
- ‚úÖ Clear error messages
- ‚úÖ Minimal configuration required

### Gate 4: Extensibility (—Ä–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç—å)

- ‚úÖ Plugin architecture –¥–ª—è –Ω–æ–≤—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
- ‚úÖ Custom metrics support
- ‚úÖ API –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π

### Gate 5: Testability (—Ç–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å)

- ‚úÖ Coverage ‚â•80% –¥–ª—è –Ω–æ–≤—ã—Ö –º–æ–¥—É–ª–µ–π
- ‚úÖ Property-based tests –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
- ‚úÖ E2E tests –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–æ–≤

---

## [ùí´] Options: –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–∏–π

### Tier 1: –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï (–≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–µ—Ä–≤—ã–º–∏)

#### üî¥ T1.1: Refactor Top-5 Complex Modules

**–¶–µ–ª—å**: –°–Ω–∏–∑–∏—Ç—å complexity –≤ core –º–æ–¥—É–ª—è—Ö (ŒîQ = +589.0)

**–ó–∞–¥–∞—á–∏**:

1. **history.py** (CCN=35, ŒîQ=+153):
   - Refactor `_run_pydriller()`: split into `_collect_commits()`, `_process_commit()`, `_aggregate_stats()`
   - Refactor `_run_git()`: extract `_parse_git_blame()`, `_compute_ownership()`
   - Target: CCN=35‚Üí<15 per function

2. **jsonld.py** (CCN=33, ŒîQ=+146):
   - Refactor `to_jsonld()`: split into `_create_context()`, `_serialize_project()`, `_serialize_files()`, `_serialize_issues()`
   - Introduce builder pattern for JSON-LD nodes
   - Target: CCN=33‚Üí<15

3. **cli.py** (CCN=26, ŒîQ=+108):
   - Refactor `_run_command()`: extract `_prepare_command()`, `_execute_process()`, `_handle_result()`
   - Refactor `_run_trs_verification()`: split verification logic
   - Split cli.py: commands/ module (analyze, gate, refactor-plan, etc.)
   - Target: CCN=26‚Üí<15, LOC=1535‚Üí<500 per file

4. **gate.py** (CCN=23, ŒîQ=+93):
   - Refactor `format_gate_report()`: extract formatters (markdown, json, junit)
   - Refactor `run_quality_gate()`: split predicates, policy evaluation
   - Target: CCN=23‚Üí<15

5. **refactoring.py** (CCN=21, ŒîQ=+89):
   - Refactor `generate_plan()`: extract PCE steps
   - Split priority calculation, ŒîQ estimation, task generation
   - Target: CCN=21‚Üí<15

**Effort**: 20-40 hours (4-8h per module)  
**Priority**: üî¥ CRITICAL  
**ŒîQ**: +589.0 points  
**Gates**: Soundness (maintain behavior), Performance (<10% slowdown), Testability (add tests for extracted functions)

#### üî¥ T1.2: Fix tmp/ Pollution & Stale Data Detection

**–¶–µ–ª—å**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö/–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

**–ó–∞–¥–∞—á–∏**:

1. **–£–ª—É—á—à–∏—Ç—å --exclude logic**:

   ```python
   # repoq/analyzers/structure.py
   def _should_exclude(self, path: Path) -> bool:
       # FIX: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –í–°–ï exclude_globs, –≤–∫–ª—é—á–∞—è tmp/**
       for pattern in self.cfg.exclude_globs:
           if path.match(pattern):
               return True
       # Add: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ —Ç–∏–ø–∏—á–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
       auto_exclude = ["tmp", "temp", ".cache", "__pycache__", "node_modules"]
       if any(part in path.parts for part in auto_exclude):
           return True
       return False
   ```

2. **Stale data warning**:

   ```python
   # repoq/core/repo_loader.py
   def analyze_project(self, path: Path) -> Project:
       # Check if analyzing potentially stale files
       stale_paths = []
       for file_path in self.collected_files:
           mtime = file_path.stat().st_mtime
           if time.time() - mtime > 86400 * 7:  # >7 days old
               stale_paths.append(file_path)
       
       if stale_paths:
           logger.warning(
               f"‚ö†Ô∏è  Analyzing {len(stale_paths)} potentially stale files "
               f"(not modified in 7+ days). Consider excluding old snapshots."
           )
   ```

3. **Auto-cleanup tmp/**:

   ```python
   # repoq/cli.py
   def _cleanup_temp_files(self, max_age_days: int = 7):
       """Remove tmp/ files older than max_age_days."""
       tmp_dirs = [Path("tmp"), Path(".repoq_cache"), Path(".repoq_tmp")]
       for tmp_dir in tmp_dirs:
           if tmp_dir.exists():
               # ... cleanup logic ...
   ```

**Effort**: 4-6 hours  
**Priority**: üî¥ CRITICAL  
**Impact**: Prevent metric pollution (–∫–∞–∫ –≤ Task #2)  
**Gates**: Soundness (don't exclude valid files), Performance (fast glob matching)

#### üî¥ T1.3: Per-Function ŒîQ Estimation

**–¶–µ–ª—å**: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –æ–∂–∏–¥–∞–µ–º—ã–π –ø—Ä–∏—Ä–æ—Å—Ç Q-score –ø—Ä–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏

**–ó–∞–¥–∞—á–∏**:

1. **Extend FunctionMetrics**:

   ```python
   @dataclass
   class FunctionMetrics:
       # ... existing fields ...
       expected_delta_q: Optional[float] = None  # ‚Üê NEW
       refactoring_priority: Optional[str] = None  # "critical", "high", "medium", "low"
   ```

2. **Compute per-function ŒîQ**:

   ```python
   # repoq/refactoring.py
   def _estimate_function_delta_q(func: FunctionMetrics, file_loc: int) -> float:
       """
       Estimate ŒîQ improvement from refactoring a function.
       
       Formula:
       ŒîQ = (current_CCN - target_CCN) * weight_complexity
            + (current_LOC - target_LOC) * weight_loc
            + file_impact_factor
       """
       target_ccn = 10.0
       target_loc = func.lines_of_code * 0.7  # Aim for 30% reduction
       
       delta_ccn = max(0, func.cyclomatic_complexity - target_ccn)
       delta_loc = max(0, func.lines_of_code - target_loc)
       
       # Weights
       w_ccn = 5.0
       w_loc = 0.5
       
       # File impact (larger files benefit more from extraction)
       file_factor = 1.0 + (file_loc / 1000.0)
       
       delta_q = (delta_ccn * w_ccn + delta_loc * w_loc) * file_factor
       return round(delta_q, 1)
   ```

3. **Update refactor-plan output**:

   ```markdown
   ### Task #3: repoq/cli.py
   **Priority**: üî¥ CRITICAL
   **Expected ŒîQ**: +108.0 points (file-level)
   
   **Recommendations**:
   1. üéØ Refactor function `_run_command` (CCN=26, lines 593-772)
      ‚Üí Expected ŒîQ: +85.0 points (78% of file's potential)
      ‚Üí Estimated effort: 3-4 hours
      
   2. üéØ Refactor function `_run_trs_verification` (CCN=16, lines 775-843)
      ‚Üí Expected ŒîQ: +15.0 points (14% of file's potential)
      ‚Üí Estimated effort: 1-2 hours
   ```

**Effort**: 6-8 hours  
**Priority**: üî¥ CRITICAL  
**Impact**: Better prioritization (refactor high-impact functions first)  
**Gates**: Soundness (ŒîQ formula validated), Usability (clear metrics)

---

### Tier 2: –í–ê–ñ–ù–´–ï (–≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ—Å–ª–µ Tier 1)

#### üü° T2.1: IDE Integration (VSCode Extension)

**–¶–µ–ª—å**: Inline refactoring recommendations –≤ —Ä–µ–¥–∞–∫—Ç–æ—Ä–µ

**–ó–∞–¥–∞—á–∏**:

1. **VSCode extension skeleton**:

   ```typescript
   // extension.ts
   export function activate(context: vscode.ExtensionContext) {
       // Command: Run RepoQ analysis
       context.subscriptions.push(
           vscode.commands.registerCommand('repoq.analyze', async () => {
               const analysis = await runRepoQAnalysis();
               showRecommendations(analysis);
           })
       );
       
       // CodeLens: Show complexity inline
       context.subscriptions.push(
           vscode.languages.registerCodeLensProvider('python', new ComplexityCodeLensProvider())
       );
   }
   ```

2. **Features**:
   - ‚úÖ Show CCN inline above functions (CodeLens)
   - ‚úÖ Quick-jump to refactoring plan (click on recommendation)
   - ‚úÖ Inline actions: "Extract function", "Split function"
   - ‚úÖ Diff preview for suggested refactorings

3. **Protocol**:

   ```json
   // LSP extension –¥–ª—è RepoQ
   {
       "method": "repoq/getRecommendations",
       "params": {
           "uri": "file:///path/to/file.py",
           "position": { "line": 593, "character": 0 }
       },
       "result": {
           "recommendations": [
               {
                   "type": "refactor.extract",
                   "title": "Extract subprocess logic",
                   "range": { "start": { "line": 600, "character": 4 }, "end": { ... } },
                   "expectedDeltaQ": 45.0
               }
           ]
       }
   }
   ```

**Effort**: 15-20 hours  
**Priority**: üü° HIGH  
**Impact**: Better developer experience (no context switching)  
**Gates**: Usability (non-intrusive), Performance (fast inline hints)

#### üü° T2.2: Multi-Language Support

**–¶–µ–ª—å**: –ê–Ω–∞–ª–∏–∑ JavaScript/TypeScript, Java, Go

**–ó–∞–¥–∞—á–∏**:

1. **Abstract complexity analyzer**:

   ```python
   # repoq/analyzers/complexity_base.py
   from abc import ABC, abstractmethod
   
   class ComplexityAnalyzerBase(ABC):
       @abstractmethod
       def analyze_file(self, path: Path) -> List[FunctionMetrics]:
           pass
       
       @abstractmethod
       def supported_extensions(self) -> List[str]:
           pass
   ```

2. **Language-specific implementations**:

   ```python
   # repoq/analyzers/complexity_js.py
   class JavaScriptComplexityAnalyzer(ComplexityAnalyzerBase):
       def analyze_file(self, path: Path) -> List[FunctionMetrics]:
           # Use ESLint's complexity plugin
           result = subprocess.run(
               ["eslint", "--format=json", "--rule", "complexity:2", str(path)],
               capture_output=True
           )
           # Parse ESLint output...
   
   # repoq/analyzers/complexity_java.py
   class JavaComplexityAnalyzer(ComplexityAnalyzerBase):
       def analyze_file(self, path: Path) -> List[FunctionMetrics]:
           # Use PMD or Checkstyle
           # ...
   ```

3. **Auto-detect language**:

   ```python
   # repoq/analyzers/factory.py
   ANALYZERS = {
       "python": PythonComplexityAnalyzer,
       "javascript": JavaScriptComplexityAnalyzer,
       "typescript": TypeScriptComplexityAnalyzer,
       "java": JavaComplexityAnalyzer,
   }
   
   def get_analyzer(language: str) -> ComplexityAnalyzerBase:
       return ANALYZERS.get(language, PythonComplexityAnalyzer)()
   ```

**Effort**: 20-30 hours (10h per language)  
**Priority**: üü° HIGH  
**Impact**: Wider adoption (not Python-only)  
**Gates**: Soundness (metrics match language idioms), Extensibility (easy to add new languages)

#### üü° T2.3: Auto-Refactoring Suggestions

**–¶–µ–ª—å**: –ü—Ä–µ–¥–ª–∞–≥–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫–æ–¥–∞

**–ó–∞–¥–∞—á–∏**:

1. **Pattern detection**:

   ```python
   # repoq/refactoring/patterns.py
   
   @dataclass
   class RefactoringPattern:
       name: str
       condition: Callable[[FunctionMetrics, str], bool]  # (metrics, source_code) -> bool
       suggestion: str
       transformations: List[Transformation]
   
   PATTERNS = [
       RefactoringPattern(
           name="extract_subprocess_logic",
           condition=lambda m, src: m.cyclomatic_complexity >= 15 and "subprocess." in src,
           suggestion="Extract subprocess handling into helper function",
           transformations=[
               ExtractFunctionTransformation(
                   pattern=r"subprocess\.(run|Popen|check_output).*?\n",
                   new_function_name="_run_subprocess",
               )
           ]
       ),
       RefactoringPattern(
           name="split_nested_conditionals",
           condition=lambda m, src: m.max_nesting_depth >= 5,
           suggestion="Split deeply nested conditionals into separate functions",
           transformations=[...]
       ),
   ]
   ```

2. **AST-based transformations**:

   ```python
   # repoq/refactoring/transformations.py
   import ast
   
   class ExtractFunctionTransformation:
       def apply(self, tree: ast.AST, target: FunctionMetrics) -> ast.AST:
           # Find target function node
           # Extract complex block
           # Create new function definition
           # Replace original block with function call
           # ...
   ```

3. **Generate diff preview**:

   ```python
   # repoq/refactoring.py
   def generate_refactoring_diff(
       file_path: Path,
       func: FunctionMetrics,
       pattern: RefactoringPattern
   ) -> str:
       """Generate unified diff for suggested refactoring."""
       original = file_path.read_text()
       transformed = apply_transformations(original, func, pattern.transformations)
       return difflib.unified_diff(original, transformed, lineterm="")
   ```

**Effort**: 30-40 hours  
**Priority**: üü° HIGH  
**Impact**: From "what to refactor" to "how to refactor"  
**Gates**: Soundness (preserve behavior), Testability (property-based tests)

---

### Tier 3: –ü–û–õ–ï–ó–ù–´–ï (–≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ—Å–ª–µ Tier 2)

#### üü¢ T3.1: Machine Learning for Smell Detection

**–¶–µ–ª—å**: –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—å code smells –ø–æ–º–∏–º–æ –º–µ—Ç—Ä–∏–∫

**–ó–∞–¥–∞—á–∏**:

1. **Dataset creation**:
   - Collect 1000+ Python repositories
   - Label code smells manually (God Class, Feature Envy, Long Method, etc.)
   - Extract features: AST patterns, metrics, naming conventions

2. **Model training**:

   ```python
   # repoq/ml/smell_detector.py
   from sklearn.ensemble import RandomForestClassifier
   
   class CodeSmellDetector:
       def __init__(self):
           self.model = RandomForestClassifier(n_estimators=100)
       
       def extract_features(self, func: FunctionMetrics, source: str) -> np.ndarray:
           # Features: CCN, LOC, params, nesting, token_count
           # + AST features: num_classes, num_loops, num_exceptions
           # + naming: function_name_length, camelCase vs snake_case
           # ...
       
       def predict_smells(self, func: FunctionMetrics, source: str) -> List[str]:
           features = self.extract_features(func, source)
           probas = self.model.predict_proba([features])[0]
           return [smell for smell, p in zip(SMELL_CLASSES, probas) if p > 0.7]
   ```

3. **Integration**:

   ```markdown
   ### Task #3: repoq/cli.py
   **Recommendations**:
   1. üéØ Refactor function `_run_command` (CCN=26)
      ü§ñ Detected smells:
         - Long Method (LOC=122, threshold=50)
         - Feature Envy (high coupling with subprocess module)
         - Primitive Obsession (many string parameters)
   ```

**Effort**: 40-60 hours (dataset + training + integration)  
**Priority**: üü¢ MEDIUM  
**Impact**: Find issues beyond complexity metrics  
**Gates**: Soundness (low false positive rate <10%), Performance (fast inference)

#### üü¢ T3.2: Quality Certificates & Badges

**–¶–µ–ª—å**: –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–æ–≤

**–ó–∞–¥–∞—á–∏**:

1. **Certificate schema**:

   ```json
   {
       "@context": "https://field33.com/ontologies/quality-cert/",
       "@type": "QualityCertificate",
       "project": "repoq-pro-final",
       "issuer": "RepoQ v0.5.0",
       "issuedAt": "2025-10-25T10:00:00Z",
       "validUntil": "2026-10-25T10:00:00Z",
       "grade": "A",
       "qScore": 85.5,
       "criteria": {
           "complexity": { "score": 90, "threshold": 80, "passed": true },
           "coverage": { "score": 63, "threshold": 80, "passed": false },
           "documentation": { "score": 95, "threshold": 70, "passed": true }
       },
       "badges": [
           "https://img.shields.io/badge/RepoQ-Grade_A-brightgreen",
           "https://img.shields.io/badge/Q--Score-85.5-green"
       ]
   }
   ```

2. **SVG badge generation**:

   ```python
   # repoq/reporting/badges.py
   def generate_badge(label: str, value: str, color: str) -> str:
       """Generate SVG badge."""
       # Use shields.io style
       # ...
   ```

3. **CLI command**:

   ```bash
   repoq certify . -o quality-certificate.json
   repoq badge . --metric q-score -o badge.svg
   ```

**Effort**: 10-15 hours  
**Priority**: üü¢ MEDIUM  
**Impact**: Showcase project quality (README badges)  
**Gates**: Usability (beautiful badges), Standards (W3C verifiable credentials)

#### üü¢ T3.3: SPARQL Query Interface

**–¶–µ–ª—å**: –ó–∞–ø—Ä–æ—Å—ã –∫ RDF/Turtle —ç–∫—Å–ø–æ—Ä—Ç—É

**–ó–∞–¥–∞—á–∏**:

1. **Local SPARQL endpoint**:

   ```python
   # repoq/semantic/sparql_server.py
   from rdflib import Graph
   from flask import Flask, request, jsonify
   
   app = Flask(__name__)
   graph = Graph()
   
   @app.route('/sparql', methods=['GET', 'POST'])
   def sparql_query():
       query = request.args.get('query') or request.form.get('query')
       results = graph.query(query)
       return jsonify([dict(row.asdict()) for row in results])
   ```

2. **Example queries**:

   ```sparql
   # Find all files with complexity > 20
   PREFIX repo: <https://field33.com/ontologies/repoq/>
   SELECT ?file ?complexity WHERE {
       ?file a repo:File ;
             repo:complexity ?complexity .
       FILTER (?complexity > 20)
   }
   ORDER BY DESC(?complexity)
   
   # Find functions with CCN > 15 and LOC > 100
   PREFIX repo: <https://field33.com/ontologies/repoq/>
   SELECT ?file ?function ?ccn ?loc WHERE {
       ?file repo:hasFunction ?function .
       ?function repo:cyclomaticComplexity ?ccn ;
                 repo:linesOfCode ?loc .
       FILTER (?ccn > 15 && ?loc > 100)
   }
   ```

3. **CLI integration**:

   ```bash
   repoq sparql . --query "SELECT ?file WHERE { ?file repo:complexity ?c . FILTER(?c > 20) }"
   repoq sparql . --server --port 8080  # Start SPARQL endpoint
   ```

**Effort**: 12-15 hours  
**Priority**: üü¢ MEDIUM  
**Impact**: Advanced semantic queries (research, BI)  
**Gates**: Standards (SPARQL 1.1 compliance), Performance (fast queries)

---

### Tier 4: –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê–õ–¨–ù–´–ï (–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ)

#### üîµ T4.1: TRS-Based Self-Verification

**–¶–µ–ª—å**: –§–æ—Ä–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ TRS

**–ó–∞–¥–∞—á–∏**:

1. **Define TRS for Python AST**:

   ```lean4
   -- repoq/verification/python_trs.lean
   inductive PyExpr where
     | Call (func : String) (args : List PyExpr)
     | Lambda (params : List String) (body : PyExpr)
     | If (cond : PyExpr) (then_ : PyExpr) (else_ : PyExpr)
     -- ...
   
   -- Rewrite rules
   def extract_function : PyExpr ‚Üí PyExpr
     | If cond (Call f args) else_ => 
         let helper := Lambda ["x"] (Call f args)
         If cond (Call "helper" [cond]) else_
   ```

2. **Prove equivalence**:

   ```lean4
   theorem extract_function_preserves_semantics (e : PyExpr) :
     eval e = eval (extract_function e) := by
       -- Proof by structural induction
       -- ...
   ```

3. **Generate verified patches**:

   ```bash
   repoq verify-refactor --file cli.py --function _run_command --strategy extract_subprocess
   # Output: Verified patch with proof certificate
   ```

**Effort**: 60-80 hours (requires Lean4 expertise)  
**Priority**: üîµ LOW (research)  
**Impact**: Provably correct refactorings  
**Gates**: Soundness (theorem prover validates), Completeness (covers common patterns)

#### üîµ T4.2: Temporal Coupling Analysis

**–¶–µ–ª—å**: –ù–∞–π—Ç–∏ —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –º–µ–Ω—è—é—Ç—Å—è –≤–º–µ—Å—Ç–µ (structural coupling)

**–ó–∞–¥–∞—á–∏**:

1. **Git log mining**:

   ```python
   # repoq/analyzers/coupling.py
   def analyze_temporal_coupling(repo_path: Path) -> Dict[Tuple[str, str], float]:
       """
       Compute temporal coupling score for file pairs.
       
       Score = commits_together / min(commits_A, commits_B)
       """
       commits = git.log("--all", "--format=%H")
       
       coupling = defaultdict(int)
       for commit_hash in commits:
           files_changed = git.diff_tree("--no-commit-id", "--name-only", "-r", commit_hash)
           for f1, f2 in combinations(files_changed, 2):
               coupling[(f1, f2)] += 1
       
       # Normalize by individual file change frequency
       # ...
   ```

2. **Visualize as graph**:

   ```python
   # repoq/reporting/coupling_graph.py
   def generate_coupling_graph(coupling: Dict, threshold: float = 0.3) -> str:
       """Generate DOT graph of highly coupled files."""
       dot = ["digraph G {"]
       for (f1, f2), score in coupling.items():
           if score >= threshold:
               dot.append(f'  "{f1}" -> "{f2}" [label="{score:.2f}"];')
       dot.append("}")
       return "\n".join(dot)
   ```

3. **Recommendations**:

   ```markdown
   ### Warning: High Temporal Coupling Detected
   
   Files `repoq/core/model.py` and `repoq/analyzers/complexity.py` change together 85% of the time.
   
   **Recommendation**: Consider merging or creating a shared abstraction to reduce coupling.
   ```

**Effort**: 15-20 hours  
**Priority**: üîµ LOW  
**Impact**: Find architectural issues  
**Gates**: Soundness (avoid false coupling from mass refactorings)

---

## [Œõ] Aggregation: –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è

### Scoring Matrix

| Task | Soundness | Performance | Usability | Extensibility | Testability | **Total** | **Priority** |
|------|-----------|-------------|-----------|---------------|-------------|-----------|--------------|
| **T1.1: Refactor Top-5** | 10 | 8 | 9 | 7 | 10 | **44** | üî¥ CRITICAL |
| **T1.2: Fix tmp/ pollution** | 10 | 9 | 10 | 6 | 8 | **43** | üî¥ CRITICAL |
| **T1.3: Per-function ŒîQ** | 9 | 10 | 10 | 8 | 7 | **44** | üî¥ CRITICAL |
| T2.1: IDE Integration | 7 | 8 | 10 | 9 | 6 | **40** | üü° HIGH |
| T2.2: Multi-Language | 8 | 7 | 9 | 10 | 7 | **41** | üü° HIGH |
| T2.3: Auto-Refactoring | 7 | 6 | 10 | 8 | 6 | **37** | üü° HIGH |
| T3.1: ML Smell Detection | 6 | 5 | 7 | 7 | 5 | **30** | üü¢ MEDIUM |
| T3.2: Quality Certificates | 8 | 9 | 8 | 6 | 8 | **39** | üü¢ MEDIUM |
| T3.3: SPARQL Queries | 9 | 7 | 6 | 8 | 7 | **37** | üü¢ MEDIUM |
| T4.1: TRS Verification | 10 | 4 | 4 | 5 | 8 | **31** | üîµ LOW |
| T4.2: Temporal Coupling | 7 | 8 | 6 | 7 | 7 | **35** | üîµ LOW |

**–ö—Ä–∏—Ç–µ—Ä–∏–∏** (–∫–∞–∂–¥—ã–π –ø–æ —à–∫–∞–ª–µ 1-10):

- **Soundness**: –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å
- **Performance**: –í–ª–∏—è–Ω–∏–µ –Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã
- **Usability**: –£–¥–æ–±—Å—Ç–≤–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- **Extensibility**: –†–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è
- **Testability**: –ü–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

**Phase 1: Foundation** (Tier 1, 30-54 hours)

1. ‚úÖ T1.2: Fix tmp/ pollution (4-6h) ‚Äî –±–ª–æ–∫–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
2. ‚úÖ T1.3: Per-function ŒîQ (6-8h) ‚Äî —É–ª—É—á—à–∞–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—é
3. ‚úÖ T1.1: Refactor Top-5 (20-40h) ‚Äî —Å–Ω–∏–∂–∞–µ—Ç technical debt

**Phase 2: Usability** (Tier 2, 65-90 hours)
4. ‚úÖ T2.1: IDE Integration (15-20h) ‚Äî —É–ª—É—á—à–∞–µ—Ç UX
5. ‚úÖ T2.2: Multi-Language (20-30h) ‚Äî —Ä–∞—Å—à–∏—Ä—è–µ—Ç –∞—É–¥–∏—Ç–æ—Ä–∏—é
6. ‚úÖ T2.3: Auto-Refactoring (30-40h) ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è

**Phase 3: Advanced** (Tier 3, 37-45 hours)
7. ‚úÖ T3.2: Quality Certificates (10-15h) ‚Äî –º–∞—Ä–∫–µ—Ç–∏–Ω–≥
8. ‚úÖ T3.3: SPARQL Queries (12-15h) ‚Äî advanced use cases
9. ‚úÖ T3.1: ML Smell Detection (40-60h) ‚Äî research

**Phase 4: Research** (Tier 4, 75-100 hours)
10. ‚úÖ T4.2: Temporal Coupling (15-20h) ‚Äî —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
11. ‚úÖ T4.1: TRS Verification (60-80h) ‚Äî —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã

---

## [R] Result: Execution Plan

### Sprint 1: Fix Critical Issues (2 weeks)

**Goals**:

- ‚úÖ Fix tmp/ pollution (T1.2)
- ‚úÖ Add per-function ŒîQ estimation (T1.3)
- ‚úÖ Refactor history.py + jsonld.py (T1.1, partial)

**Deliverables**:

- ‚úÖ No stale data warnings
- ‚úÖ Per-function ŒîQ in refactoring-plan
- ‚úÖ history.py CCN: 35‚Üí<15
- ‚úÖ jsonld.py CCN: 33‚Üí<15

**Success Criteria**:

- All Tier 1 tests pass
- Coverage ‚â•70%
- ŒîQ improvement: +300 points

### Sprint 2: Complete Top-5 Refactoring (2 weeks)

**Goals**:

- ‚úÖ Refactor cli.py, gate.py, refactoring.py (T1.1, complete)

**Deliverables**:

- ‚úÖ cli.py split into commands/ module
- ‚úÖ gate.py CCN: 23‚Üí<15
- ‚úÖ refactoring.py CCN: 21‚Üí<15

**Success Criteria**:

- All Top-5 tasks completed
- Coverage ‚â•75%
- Total ŒîQ improvement: +589 points

### Sprint 3: IDE Integration (2 weeks)

**Goals**:

- ‚úÖ VSCode extension MVP (T2.1)

**Deliverables**:

- ‚úÖ Inline CCN display (CodeLens)
- ‚úÖ Quick-jump to recommendations
- ‚úÖ Basic refactoring actions

**Success Criteria**:

- Extension published to VSCode Marketplace
- ‚â•50 installs in first week
- Positive user feedback

### Sprint 4: Multi-Language Support (3 weeks)

**Goals**:

- ‚úÖ Add JavaScript/TypeScript support (T2.2)

**Deliverables**:

- ‚úÖ ESLint integration
- ‚úÖ Per-function metrics for JS/TS
- ‚úÖ refactoring-plan support for JS/TS

**Success Criteria**:

- Analyze 5+ popular JS/TS repos
- Metrics match ESLint output
- Coverage ‚â•70% for new code

### Sprint 5+: Advanced Features (ongoing)

- T2.3: Auto-Refactoring (4 weeks)
- T3.1: ML Smell Detection (6 weeks)
- T3.2: Quality Certificates (2 weeks)
- T3.3: SPARQL Queries (2 weeks)

---

## Immediate Next Steps (Next 48 hours)

### Step 1: Fix tmp/ Pollution ‚úÖ URGENT

```bash
# 1. Remove all tmp/ directories
rm -rf tmp/

# 2. Add .gitignore entry
echo "tmp/" >> .gitignore

# 3. Fix exclude logic in structure.py
# (—Å–º. T1.2 –≤—ã—à–µ)

# 4. Run fresh analysis
repoq analyze . -o baseline-clean.jsonld --extensions py

# 5. Verify no tmp/ files
python3 -c "
import json
data = json.load(open('baseline-clean.jsonld'))
tmp_files = [f['path'] for f in data.get('files', []) if 'tmp/' in f['path']]
assert len(tmp_files) == 0, f'Found {len(tmp_files)} tmp/ files!'
print('‚úÖ No tmp/ pollution!')
"
```

### Step 2: Start T1.1 - Refactor history.py ‚úÖ HIGH PRIORITY

```bash
# 1. Create tracking document
touch docs/vdad/task-t1.1-history-refactoring.md

# 2. Baseline measurement
repoq analyze . -o baseline-history.jsonld
# Extract history.py metrics: _run_pydriller CCN=35, _run_git CCN=30

# 3. Apply refactoring plan
# - Extract _collect_commits() from _run_pydriller()
# - Extract _process_commit() from _run_pydriller()
# - Extract _parse_git_blame() from _run_git()

# 4. Measure after each step
repoq analyze . -o after-history-step1.jsonld
# Verify CCN reduction

# 5. Run tests
pytest tests/analyzers/test_history.py -v

# 6. Commit
git add repoq/analyzers/history.py tests/
git commit -m "refactor(history): extract functions (T1.1 Step 1)

- Extract _collect_commits() from _run_pydriller()
- Reduce CCN: 35‚Üí28
- Add tests for extracted function

Ref: docs/vdad/improvement-roadmap.md (T1.1)"
```

### Step 3: Implement T1.3 - Per-Function ŒîQ ‚úÖ HIGH PRIORITY

```bash
# 1. Extend FunctionMetrics dataclass
# (—Å–º. T1.3 –≤—ã—à–µ)

# 2. Add _estimate_function_delta_q() to refactoring.py

# 3. Update generate_recommendations() to show per-function ŒîQ

# 4. Test
repoq analyze . -o test.jsonld
repoq refactor-plan test.jsonld -o plan-with-delta-q.md
# Verify output shows per-function ŒîQ

# 5. Commit
git add repoq/core/model.py repoq/refactoring.py
git commit -m "feat: Add per-function ŒîQ estimation (T1.3)

- Extend FunctionMetrics with expected_delta_q field
- Compute ŒîQ based on CCN reduction + LOC + file impact
- Show per-function ŒîQ in refactor-plan

Example:
'Refactor function _run_command (CCN=26) ‚Üí ŒîQ: +85.0 (78% of file)'

Ref: docs/vdad/improvement-roadmap.md (T1.3)"
```

---

## Success Metrics (Q4 2025)

**Code Quality**:

- ‚úÖ Average file CCN: <15 (current: ~25)
- ‚úÖ Max function CCN: <20 (current: 35)
- ‚úÖ Test coverage: ‚â•80% (current: 63%)

**Features**:

- ‚úÖ Per-function ŒîQ estimation
- ‚úÖ IDE integration (VSCode)
- ‚úÖ Multi-language support (Python + JS/TS)

**Adoption**:

- ‚úÖ ‚â•100 VSCode extension installs
- ‚úÖ ‚â•10 external projects using RepoQ
- ‚úÖ ‚â•5 contributors

**Documentation**:

- ‚úÖ Complete API docs
- ‚úÖ Tutorial: "Refactoring with RepoQ"
- ‚úÖ Case study: "Dogfooding meta-level"

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

**Roadmap summary**:

- **Tier 1** (CRITICAL): Fix technical debt + foundation (30-54h)
- **Tier 2** (HIGH): Usability + extensibility (65-90h)
- **Tier 3** (MEDIUM): Advanced features (37-45h)
- **Tier 4** (LOW): Research + experiments (75-100h)

**Total effort**: 207-289 hours (~5-7 months –ø—Ä–∏ 10h/week)

**Next immediate actions**:

1. ‚úÖ Fix tmp/ pollution (4-6h) ‚Äî URGENT
2. ‚úÖ Refactor history.py (8-10h) ‚Äî HIGH
3. ‚úÖ Add per-function ŒîQ (6-8h) ‚Äî HIGH

**Philosophy**:
> "A system for synthesis should be able to synthesize improvements to itself."

–≠—Ç–æ—Ç roadmap ‚Äî –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è **—Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–π –ø–æ–ª–Ω–æ—Ç—ã**: RepoQ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–±—è ‚Üí –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã ‚Üí –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–ª–∞–Ω —É–ª—É—á—à–µ–Ω–∏–π ‚Üí —Ä–µ–∞–ª–∏–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è ‚Üí —Å–Ω–æ–≤–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–±—è.

**TRUE meta-level dogfooding!** üöÄ

---

**–ê–≤—Ç–æ—Ä**: AI Senior Engineer (Œ£‚ÜíŒì‚Üíùí´‚ÜíŒõ‚ÜíR methodology)  
**–î–∞—Ç–∞**: 2025-10-22  
**–í–µ—Ä—Å–∏—è**: 1.0  
**–°—Ç–∞—Ç—É—Å**: READY FOR EXECUTION  
**Tracking**: docs/vdad/improvement-roadmap.md
