"""Refactoring task generation based on PCE (Proof of Correct Execution) algorithm.

This module implements automated refactoring task generation using greedy
k-repair selection based on ŒîQ impact estimation.

Algorithm:
    1. Load baseline quality metrics from JSON-LD
    2. Calculate ŒîQ for each file (impact of fixing issues)
    3. Greedily select top-k files by ŒîQ impact
    4. Generate actionable tasks with specific recommendations

Formula:
    ŒîQ(file) = Œ£(w_i √ó penalty_i)
    where:
        w_complexity = 5.0 (high impact on maintainability)
        w_todos = 2.0 (technical debt indicators)
        w_issues = 3.0 (quality issues)
        w_hotspot = 4.0 (change frequency risk)
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import List


@dataclass
class RefactoringTask:
    """A single refactoring task with priority and recommendations."""

    id: int
    file_path: str
    priority: str  # "critical", "high", "medium", "low"
    delta_q: float  # Expected quality improvement
    current_metrics: dict
    issues: List[str]
    recommendations: List[str]
    estimated_effort: str  # "15 min", "1 hour", "2-4 hours"

    def to_dict(self) -> dict:
        """Convert task to dictionary for JSON export."""
        return {
            "id": self.id,
            "file_path": self.file_path,
            "priority": self.priority,
            "delta_q": round(self.delta_q, 2),
            "current_metrics": self.current_metrics,
            "issues": self.issues,
            "recommendations": self.recommendations,
            "estimated_effort": self.estimated_effort,
        }

    def to_markdown(self) -> str:
        """Format task as Markdown."""
        priority_emoji = {
            "critical": "üî¥",
            "high": "üü†",
            "medium": "üü°",
            "low": "üü¢",
        }

        lines = [
            f"### Task #{self.id}: {self.file_path}",
            f"**Priority**: {priority_emoji.get(self.priority, '‚ö™')} {self.priority.upper()}",
            f"**Expected ŒîQ**: +{self.delta_q:.1f} points",
            f"**Estimated effort**: {self.estimated_effort}",
            "",
            "**Current metrics**:",
        ]

        for key, value in self.current_metrics.items():
            lines.append(f"- {key}: {value}")

        lines.extend(["", "**Issues**:"])
        for issue in self.issues:
            lines.append(f"- {issue}")

        lines.extend(["", "**Recommendations**:"])
        for i, rec in enumerate(self.recommendations, 1):
            lines.append(f"{i}. {rec}")

        lines.append("")
        return "\n".join(lines)

    def to_github_issue(self, repo_name: str = "repoq") -> dict:
        """Format task as GitHub Issue payload."""
        labels = [self.priority, "refactoring", "tech-debt"]

        body_lines = [
            f"## üìã Refactoring Task: `{self.file_path}`",
            "",
            f"**Expected quality improvement**: ŒîQ = +{self.delta_q:.1f}",
            f"**Estimated effort**: {self.estimated_effort}",
            "",
            "### Current Metrics",
            "",
        ]

        for key, value in self.current_metrics.items():
            body_lines.append(f"- **{key}**: {value}")

        body_lines.extend(["", "### Issues Detected", ""])
        for issue in self.issues:
            body_lines.append(f"- [ ] {issue}")

        body_lines.extend(["", "### Recommended Actions", ""])
        for i, rec in enumerate(self.recommendations, 1):
            body_lines.append(f"{i}. {rec}")

        body_lines.extend(
            [
                "",
                "---",
                "*Generated by RepoQ refactor-plan command*",
            ]
        )

        return {
            "title": f"Refactor: {Path(self.file_path).name} (ŒîQ: +{self.delta_q:.1f})",
            "body": "\n".join(body_lines),
            "labels": labels,
        }


@dataclass
class RefactoringPlan:
    """Complete refactoring plan with multiple tasks."""

    tasks: List[RefactoringTask]
    total_delta_q: float
    baseline_q: float
    projected_q: float

    def to_markdown(self) -> str:
        """Format plan as Markdown report."""
        lines = [
            "# üîß Refactoring Plan",
            "",
            "## Executive Summary",
            "",
            f"- **Current Q-score**: {self.baseline_q:.2f}",
            f"- **Projected Q-score**: {self.projected_q:.2f}",
            f"- **Total ŒîQ**: +{self.total_delta_q:.1f}",
            f"- **Tasks**: {len(self.tasks)}",
            "",
            "## Priority Breakdown",
            "",
        ]

        # Count by priority
        priority_counts = {}
        for task in self.tasks:
            priority_counts[task.priority] = priority_counts.get(task.priority, 0) + 1

        for priority in ["critical", "high", "medium", "low"]:
            count = priority_counts.get(priority, 0)
            if count > 0:
                lines.append(f"- **{priority.capitalize()}**: {count} tasks")

        lines.extend(["", "## Tasks", ""])

        for task in self.tasks:
            lines.append(task.to_markdown())

        lines.extend(
            [
                "---",
                "",
                "## Execution Strategy",
                "",
                "1. **Critical tasks**: Fix immediately (blocking issues)",
                "2. **High priority**: Include in current sprint",
                "3. **Medium priority**: Plan for next sprint",
                "4. **Low priority**: Backlog for future iterations",
                "",
                "**Success criteria**:",
                f"- ‚úÖ Q-score ‚â• {self.projected_q:.2f}",
                "- ‚úÖ All critical tasks resolved",
                "- ‚úÖ No regression in test coverage",
                "- ‚úÖ PCQ ‚â• 0.8 (gaming resistance)",
            ]
        )

        return "\n".join(lines)


def calculate_delta_q(file_data: dict) -> float:
    """Calculate expected ŒîQ if file issues are fixed.

    Args:
        file_data: File metrics dictionary from JSON-LD

    Returns:
        Expected quality score improvement (ŒîQ)

    Weights based on Phase 4 Q-formula:
        - Complexity: 5.0 (high maintainability impact)
        - TODOs: 2.0 (tech debt indicators)
        - Issues: 3.0 (quality problems)
        - Hotspot score: 4.0 (change frequency risk)
    """
    complexity = file_data.get("complexity", 0) or 0
    todos = len(file_data.get("todos", [])) if isinstance(file_data.get("todos"), list) else 0
    issues = len(file_data.get("issues", [])) if isinstance(file_data.get("issues"), list) else 0
    hotspot_score = file_data.get("hotspotScore", 0) or 0

    # Weights from Q-formula
    w_complexity = 5.0
    w_todos = 2.0
    w_issues = 3.0
    w_hotspot = 4.0

    # Calculate penalties (higher = worse)
    complexity_penalty = max(0, complexity - 5)  # Penalty if complexity > 5
    todo_penalty = todos
    issue_penalty = issues
    hotspot_penalty = max(0, hotspot_score - 50)  # Penalty if hotspot > 50

    delta_q = (
        w_complexity * complexity_penalty
        + w_todos * todo_penalty
        + w_issues * issue_penalty
        + w_hotspot * (hotspot_penalty / 10.0)  # Scale down hotspot
    )

    return delta_q


def _estimate_function_delta_q(func: dict, file_loc: int) -> float:
    """Estimate ŒîQ improvement from refactoring a single function (T1.3).
    
    Args:
        func: Function metrics dict with cyclomaticComplexity, linesOfCode, etc.
        file_loc: Total file LOC for file impact factor
    
    Returns:
        Expected ŒîQ improvement (0.0 if function doesn't need refactoring)
    
    Formula:
        ŒîQ = (CCN_delta √ó w_ccn + LOC_delta √ó w_loc) √ó file_factor
        where:
            CCN_delta = max(0, current_CCN - target_CCN)
            LOC_delta = max(0, current_LOC - target_LOC)
            file_factor = 1.0 + (file_LOC / 1000.0)  # Larger files benefit more
    """
    current_ccn = func.get("cyclomaticComplexity", 0)
    current_loc = func.get("linesOfCode", 0)
    
    # Target thresholds
    target_ccn = 10.0
    target_loc = current_loc * 0.7  # Aim for 30% reduction
    
    # Calculate deltas (only positive improvements)
    ccn_delta = max(0, current_ccn - target_ccn)
    loc_delta = max(0, current_loc - target_loc)
    
    # Weights
    w_ccn = 5.0  # High weight for complexity reduction
    w_loc = 0.5  # Lower weight for LOC reduction
    
    # File impact factor (larger files benefit more from extraction)
    file_factor = 1.0 + (file_loc / 1000.0)
    
    # Calculate ŒîQ
    delta_q = (ccn_delta * w_ccn + loc_delta * w_loc) * file_factor
    
    return round(delta_q, 1)


def _get_function_priority(delta_q: float) -> str:
    """Determine refactoring priority based on ŒîQ (T1.3).
    
    Args:
        delta_q: Expected ŒîQ improvement
    
    Returns:
        Priority level: "critical", "high", "medium", or "low"
    """
    if delta_q >= 80:
        return "critical"
    elif delta_q >= 40:
        return "high"
    elif delta_q >= 20:
        return "medium"
    else:
        return "low"


def _generate_function_recommendations(
    functions: List[dict], loc: int, complexity: float
) -> List[str]:
    """Generate per-function refactoring recommendations with ŒîQ estimation.
    
    Args:
        functions: List of function metrics dicts
        loc: Total file LOC for impact calculation
        complexity: File-level complexity for fallback validation
    
    Returns:
        List of formatted recommendations for complex functions
    """
    recommendations = []
    complex_funcs = [f for f in functions if f.get("cyclomaticComplexity", 0) >= 10]
    
    if not complex_funcs:
        # If file complexity high but no individual function >10
        if complexity >= 10:
            recommendations.append(
                f"‚ö†Ô∏è File complexity={complexity} but no function >10 "
                "(verify metrics or refactor distributed complexity)"
            )
        return recommendations
    
    # Calculate ŒîQ for each function
    file_delta_q_total = 0.0
    for func in complex_funcs:
        delta_q = _estimate_function_delta_q(func, loc)
        func["_delta_q"] = delta_q
        file_delta_q_total += delta_q
    
    # Sort by ŒîQ descending (highest impact first)
    complex_funcs.sort(key=lambda f: f.get("_delta_q", 0), reverse=True)
    
    # Generate recommendations for top 3 highest impact functions
    for func in complex_funcs[:3]:
        fname = func.get("name", "unknown")
        fccn = func.get("cyclomaticComplexity", 0)
        flines = f"{func.get('startLine', '?')}-{func.get('endLine', '?')}"
        floc = func.get("linesOfCode", 0)
        delta_q = func.get("_delta_q", 0.0)
        
        # Calculate percentage of file's total potential
        pct = int((delta_q / file_delta_q_total * 100)) if file_delta_q_total > 0 else 0
        
        # Priority indicator
        priority = _get_function_priority(delta_q)
        priority_emoji = {
            "critical": "üî¥",
            "high": "üü†", 
            "medium": "üü°",
            "low": "üü¢"
        }.get(priority, "‚ö™")
        
        # Estimate effort
        if fccn >= 20:
            effort_str = "3-4 hours"
        elif fccn >= 15:
            effort_str = "2-3 hours"
        elif fccn >= 10:
            effort_str = "1-2 hours"
        else:
            effort_str = "30-60 min"
        
        recommendations.append(
            f"{priority_emoji} Refactor function `{fname}` "
            f"(CCN={fccn}, LOC={floc}, lines {flines})\n"
            f"   ‚Üí Expected ŒîQ: +{delta_q:.1f} points ({pct}% of file's potential)\n"
            f"   ‚Üí Estimated effort: {effort_str}"
        )
    
    return recommendations


def _generate_file_level_recommendations(
    complexity: float, loc: int, todos: int, functions: List[dict]
) -> List[str]:
    """Generate file-level refactoring recommendations (fallback when no functions).
    
    Args:
        complexity: File-level cyclomatic complexity
        loc: Lines of code
        todos: Number of TODO comments
        functions: List of function metrics (used to check if fallback needed)
    
    Returns:
        List of file-level recommendations
    """
    recommendations = []
    
    # Complexity recommendations (only if no per-function analysis)
    if not functions:
        if complexity >= 10:
            recommendations.append(
                f"üî¥ **Critical**: Reduce cyclomatic complexity from {complexity} to <10 "
                "(split into smaller functions)"
            )
        elif complexity >= 6:
            recommendations.append(
                f"üü° Extract complex logic into helper functions (current: {complexity})"
            )
    
    # LOC recommendations
    if loc > 500:
        recommendations.append(
            f"üìè Consider splitting file ({loc} LOC) into smaller modules (<300 LOC)"
        )
    
    # TODO recommendations
    if todos >= 5:
        recommendations.append(f"üìù Resolve {todos} TODO comments (prioritize blocking issues)")
    elif todos > 0:
        recommendations.append(f"üìù Address {todos} TODO comment(s)")
    
    return recommendations


def _generate_issue_recommendations(issues: List) -> List[str]:
    """Generate issue-specific refactoring recommendations.
    
    Args:
        issues: List of issue identifiers or dicts
    
    Returns:
        List of issue-specific recommendations
    """
    recommendations = []
    
    for issue in issues:
        if isinstance(issue, str):
            # Parse issue ID to get type
            if "hotspot" in issue:
                recommendations.append("üî• Reduce change frequency (refactor to stabilize)")
            elif "complexity" in issue:
                recommendations.append("üîß Simplify control flow (reduce nested logic)")
    
    return recommendations


def generate_recommendations(file_data: dict) -> List[str]:
    """Generate specific refactoring recommendations based on metrics.

    Args:
        file_data: File metrics dictionary

    Returns:
        List of actionable recommendations (limited to top 5)
    """
    # Extract metrics
    complexity = file_data.get("complexity", 0) or 0
    loc = file_data.get("linesOfCode", 0) or 0
    todos = len(file_data.get("todos", [])) if isinstance(file_data.get("todos"), list) else 0
    issues = file_data.get("issues", [])
    functions = file_data.get("functions", [])
    
    # Generate recommendations from different analyzers
    recommendations = []
    
    # 1. Per-function analysis (highest priority)
    if functions:
        recommendations.extend(_generate_function_recommendations(functions, loc, complexity))
    
    # 2. File-level metrics (LOC, TODOs, complexity fallback)
    recommendations.extend(_generate_file_level_recommendations(complexity, loc, todos, functions))
    
    # 3. Issue-specific recommendations
    recommendations.extend(_generate_issue_recommendations(issues))
    
    # 4. Default if no recommendations generated
    if not recommendations:
        recommendations.append("‚úÖ Maintain current quality (minor cleanup possible)")
    
    return recommendations[:5]  # Limit to top 5


def estimate_effort(delta_q: float, complexity: float, loc: int) -> str:
    """Estimate refactoring effort based on impact and size.

    Args:
        delta_q: Expected quality improvement
        complexity: Cyclomatic complexity
        loc: Lines of code

    Returns:
        Human-readable effort estimation
    """
    # High impact + high complexity = more effort
    effort_score = delta_q + (complexity * 0.5) + (loc / 100.0)

    if effort_score >= 40:
        return "4-8 hours (complex refactoring)"
    elif effort_score >= 20:
        return "2-4 hours (moderate refactoring)"
    elif effort_score >= 10:
        return "1-2 hours (focused refactoring)"
    else:
        return "15-30 minutes (quick fixes)"


def assign_priority(delta_q: float, complexity: float) -> str:
    """Assign priority based on ŒîQ and complexity.

    Args:
        delta_q: Expected quality improvement
        complexity: Cyclomatic complexity

    Returns:
        Priority level: "critical", "high", "medium", or "low"
    """
    if delta_q >= 30 or complexity >= 10:
        return "critical"
    elif delta_q >= 15 or complexity >= 7:
        return "high"
    elif delta_q >= 5:
        return "medium"
    else:
        return "low"


def generate_refactoring_plan(
    jsonld_path: str | Path,
    top_k: int = 10,
    min_delta_q: float = 3.0,
) -> RefactoringPlan:
    """Generate refactoring plan from analysis results.

    Args:
        jsonld_path: Path to JSON-LD quality analysis file
        top_k: Maximum number of tasks to generate (greedy selection)
        min_delta_q: Minimum ŒîQ threshold for inclusion

    Returns:
        Complete refactoring plan with prioritized tasks

    Algorithm:
        1. Load quality data from JSON-LD
        2. Calculate ŒîQ for each file
        3. Filter by min_delta_q threshold
        4. Sort by ŒîQ descending (greedy)
        5. Select top-k files
        6. Generate recommendations and estimate effort
        7. Assign priorities
    """
    # Load data
    jsonld_path = Path(jsonld_path)
    if not jsonld_path.exists():
        raise FileNotFoundError(f"JSON-LD file not found: {jsonld_path}")

    data = json.loads(jsonld_path.read_text(encoding="utf-8"))

    # Extract files
    files = data.get("files", [])
    if not isinstance(files, list):
        files = list(files.values()) if isinstance(files, dict) else []

    # Calculate ŒîQ for each file
    file_scores = []
    for file_data in files:
        delta_q = calculate_delta_q(file_data)

        if delta_q >= min_delta_q:
            file_scores.append(
                {
                    "path": file_data.get("path", file_data.get("fileName", "unknown")),
                    "delta_q": delta_q,
                    "data": file_data,
                }
            )

    # Sort by ŒîQ descending (greedy selection)
    file_scores.sort(key=lambda x: x["delta_q"], reverse=True)

    # Select top-k
    selected = file_scores[:top_k]

    # Generate tasks
    tasks = []
    total_delta_q = 0.0

    for i, item in enumerate(selected, 1):
        file_data = item["data"]
        delta_q = item["delta_q"]
        path = item["path"]

        complexity = file_data.get("complexity", 0) or 0
        loc = file_data.get("linesOfCode", 0) or 0
        todos = len(file_data.get("todos", [])) if isinstance(file_data.get("todos"), list) else 0
        issues = file_data.get("issues", [])

        current_metrics = {
            "Complexity": complexity,
            "LOC": loc,
            "TODOs": todos,
            "Issues": len(issues) if isinstance(issues, list) else 0,
        }

        # Generate issues list
        issue_descriptions = []
        if complexity >= 6:
            issue_descriptions.append(f"High cyclomatic complexity ({complexity})")
        if todos > 0:
            issue_descriptions.append(f"{todos} unresolved TODO(s)")
        if loc > 500:
            issue_descriptions.append(f"Large file size ({loc} LOC)")

        recommendations = generate_recommendations(file_data)
        effort = estimate_effort(delta_q, complexity, loc)
        priority = assign_priority(delta_q, complexity)

        task = RefactoringTask(
            id=i,
            file_path=path,
            priority=priority,
            delta_q=delta_q,
            current_metrics=current_metrics,
            issues=issue_descriptions or ["No critical issues"],
            recommendations=recommendations,
            estimated_effort=effort,
        )

        tasks.append(task)
        total_delta_q += delta_q

    # Calculate baseline and projected Q-scores
    # Try to extract from data if available, otherwise estimate
    baseline_q = data.get("qualityScore", 0.0)
    if baseline_q == 0.0:
        # Estimate baseline from penalties
        total_penalty = sum(f["delta_q"] for f in file_scores)
        baseline_q = max(0, 100.0 - (total_penalty / len(files) * 10 if files else 0))

    projected_q = min(100.0, baseline_q + total_delta_q)

    return RefactoringPlan(
        tasks=tasks,
        total_delta_q=total_delta_q,
        baseline_q=baseline_q,
        projected_q=projected_q,
    )
